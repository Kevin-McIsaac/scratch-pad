{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL TUL Utilisation data\n",
    "tul_util_df = pd.read_excel(\"data/TULNetRateByTrain.xlsx\")\n",
    "tul_util_df[\"COMMENCE_DUMPING\"] = pd.to_datetime(tul_util_df[\"COMMENCE_DUMPING\"])\n",
    "tul_util_df[\"COMPLETE_DUMPING\"] = pd.to_datetime(tul_util_df[\"COMPLETE_DUMPING\"])\n",
    "tul_util_df = tul_util_df[[\"TUL\", \"COMMENCE_DUMPING\", \"COMPLETE_DUMPING\"]].rename(columns={'COMMENCE_DUMPING': 'start', 'COMPLETE_DUMPING': 'end'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL TUL APLUS data\n",
    "tul_delay_df = pd.read_csv(\"tul_aplus_delays_fully_categorised.csv\")\n",
    "tul_delay_df[\"StartTime\"] = pd.to_datetime(tul_delay_df[\"StartTime\"])\n",
    "tul_delay_df[\"EndTime\"] = pd.to_datetime(tul_delay_df[\"EndTime\"])\n",
    "tul_delay_df[\"TUL\"] = np.where(\n",
    "    # assign TULs to each delay\n",
    "    (\n",
    "        (tul_delay_df[\"EquipmentName\"].str.startswith(\"TUL 1\")) |\\\n",
    "        (tul_delay_df[\"EquipmentName\"].str.startswith(\"TUL601\")) | \\\n",
    "         tul_delay_df[\"EquipmentName\"].str.startswith(\"InCircuit1\")\n",
    "    ), \n",
    "    \"TUL1\",\n",
    "    np.where(\n",
    "        (\n",
    "            (tul_delay_df[\"EquipmentName\"].str.startswith(\"TUL 2\")) | \\\n",
    "            (tul_delay_df[\"EquipmentName\"].str.startswith(\"TUL602\")) | \\\n",
    "             tul_delay_df[\"EquipmentName\"].str.startswith(\"InCircuit2\")\n",
    "        ),\n",
    "        \"TUL2\",\n",
    "        \"TUL3\"\n",
    "    )\n",
    ")\n",
    "tul_delay_df = tul_delay_df[[\"TUL\", \"StartTime\", \"EndTime\", \"DelayCategory\"]].rename(columns={'StartTime': 'start', 'EndTime': 'end'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create time buckets crossed with TULs\n",
    "RANGE_START = \"2023-10-01\"\n",
    "RANGE_END = \"2023-11-01\"\n",
    "TIME_WINDOW_MINS = 5\n",
    "time_buckets = pd.date_range(RANGE_START, RANGE_END, freq=f'{TIME_WINDOW_MINS} min')\n",
    "time_bucket_df = pd.DataFrame({'start': time_buckets})\n",
    "time_bucket_df[\"end\"] = time_bucket_df[\"start\"] + pd.Timedelta(TIME_WINDOW_MINS, 'minute')\n",
    "time_bucket_tul_df = pd.merge(time_bucket_df, pd.DataFrame(['TUL1', 'TUL2', 'TUL3'], columns=['TUL']), how='cross')\n",
    "\n",
    "# Filter dfs to required range\n",
    "tul_util_df = tul_util_df[\n",
    "    (tul_util_df[\"start\"] >= pd.to_datetime(RANGE_START)) &\n",
    "    (tul_util_df[\"end\"] <= pd.to_datetime(RANGE_END))\n",
    "]\n",
    "tul_delay_df = tul_delay_df[\n",
    "    (tul_delay_df[\"start\"] >= pd.to_datetime(RANGE_START)) &\n",
    "    (tul_delay_df[\"end\"] <= pd.to_datetime(RANGE_END))\n",
    "]\n",
    "\n",
    "# Perform range join on time buckets and util/delays\n",
    "# NOTE: range joining on time is a pain in pandas, so using SQL to do it\n",
    "conn = sqlite3.connect(':memory:')\n",
    "time_bucket_tul_df.to_sql('time_buckets_per_tul', conn, index=False)\n",
    "tul_util_df.to_sql('tul_transactions', conn, index=False)\n",
    "tul_delay_df.to_sql('tul_delays', conn, index=False)\n",
    "query = '''\n",
    "    select\n",
    "        A.start as time_bucket,\n",
    "        A.TUL as tul,\n",
    "        (julianday(min(A.end, B.end))-julianday(max(A.start, B.start)))*1440.0 as usage_time,\n",
    "        (julianday(min(A.end, C.end))-julianday(max(A.start, C.start)))*1440.0 as delay_time,\n",
    "        C.DelayCategory as delay_category\n",
    "    from\n",
    "        time_buckets_per_tul A\n",
    "        left join\n",
    "            tul_transactions B on (A.TUL = B.TUL) & ((julianday(min(A.end, B.end))-julianday(max(A.start, B.start)))*1440.0 > 0)\n",
    "        left join\n",
    "            tul_delays C on (A.TUL = C.TUL) & ((julianday(min(A.end, C.end))-julianday(max(A.start, C.start)))*1440.0 > 0)\n",
    "'''\n",
    "merged_df = pd.read_sql_query(query,conn)\n",
    "\n",
    "# Determine status of TUL\n",
    "merged_df[\"status\"] = np.where(\n",
    "    merged_df[\"usage_time\"] >= TIME_WINDOW_MINS/2, # must be used greater than half the interval (same definition as used last year)\n",
    "    \"used\",\n",
    "    np.where(\n",
    "        ((merged_df[\"delay_category\"] == \"Maintenance/inspection\") & (merged_df[\"delay_time\"] >= 2.5)) | ((merged_df[\"delay_category\"] == \"Standby_due_to_failure\") & (merged_df[\"delay_time\"] >= 2.5)),\n",
    "        \"unavailable\",\n",
    "        \"idle\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Pile Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(  '''\n",
    "select DATE(transaction_datetime) as transaction_date, stockpile_name, balance_wmt, transaction_datetime\n",
    "from SBX_AA_OPERATIONS_ARTIFICIALINTELLIGENCE.STG_ARTIFICIALINTELLIGENCE.kmc_stg_delta__inventory_stockpile_transactions\n",
    "where stockpile_type = 'PortStockpile' and transaction_datetime > '2024-01-01' and stockpile_name NOT LIKE '__ -%'\n",
    "qualify row_number() over (partition by stockpile_name, DATE(transaction_datetime) order by transaction_datetime desc) = 1\n",
    "order by stockpile_name,  DATE(transaction_datetime)\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"StockpileTransactions.csv\")\n",
    "# bucketed time\n",
    "times = pd.date_range(RANGE_START, RANGE_END, freq=f'1d')\n",
    "stockpiles = [f\"{canyon}{number+1}\" for canyon in [\"B\",\"C\",\"D\",\"E\",\"F\"] for number in range(6)]\n",
    "df = df[df[\"STOCKPILENAME\"].isin(stockpiles)]\n",
    "df[\"TRANSACTIONDATETIME\"] = pd.to_datetime(df[\"TRANSACTIONDATETIME\"], dayfirst=True)\n",
    "df = df[(df[\"TRANSACTIONDATETIME\"] >= pd.to_datetime(RANGE_START)) & (df[\"TRANSACTIONDATETIME\"] < pd.to_datetime(RANGE_END))]\n",
    "df[\"date\"] = df[\"TRANSACTIONDATETIME\"].dt.date\n",
    "df = df.sort_values(\"date\")\n",
    "df = df.groupby([\"STOCKPILENAME\", \"date\"]).agg(['last']).stack().reset_index()[[\"STOCKPILENAME\", \"date\", \"BALANCEWMT\"]]\n",
    "df = df.groupby(\"date\")[\"BALANCEWMT\"].sum().reset_index()\n",
    "vals = df[\"BALANCEWMT\"].quantile([0.33, 0.66])\n",
    "df[\"Stockpile_health\"] = np.where(\n",
    "    df[\"BALANCEWMT\"]>=vals[0.66],\n",
    "    \"High\",\n",
    "    np.where(\n",
    "        df[\"BALANCEWMT\"]>=vals[0.33],\n",
    "        \"Medium\",\n",
    "        \"Low\"\n",
    "    )\n",
    ")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
